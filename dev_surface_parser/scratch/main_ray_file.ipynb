{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b308e1-5963-4b1f-9f78-25577e53f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from numba import jit, njit\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "import re\n",
    "from utils import *\n",
    "from stats import Stats\n",
    "from statscols import StatsColumns\n",
    "import yaml\n",
    "import glob\n",
    "import os\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102b9dc-50c1-49a2-aaf2-12323b921958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict, data_path: str, categories: list, drop_duplicates: bool):\n",
    "    '''\n",
    "    args:\n",
    "        data_path: path to imaris file\n",
    "    '''\n",
    "    # load the data\n",
    "    full_data_file = load_data(data_path)\n",
    "\n",
    "    # get the points info inside the file\n",
    "    points = get_points(full_data_file)\n",
    "    \n",
    "    # storage to store multiple dataframes\n",
    "    dataframe_storage = list()\n",
    "\n",
    "    full_storage = {}\n",
    "    \n",
    "    # metadata storage\n",
    "    metadata_storage = {}\n",
    "\n",
    "    # loop over each point\n",
    "    for idx, point in enumerate(points):\n",
    "\n",
    "        # create a dictionary that maps the statistics name to the \n",
    "        stats_dict = get_statistics_dict(full_data_file, point)\n",
    "        \n",
    "        # create the functions dict\n",
    "        remove_list = read_txt(config['remove_list_path'])\n",
    "        functions_dict = create_functions_dict(categories, remove_list, stats_dict)\n",
    "\n",
    "        try:\n",
    "            # get the track information\n",
    "            track_id_data = get_stats(full_data_file, point, 'Track0')\n",
    "\n",
    "            # get the track object information\n",
    "            track_object_data = get_stats(full_data_file, point, 'TrackObject0')\n",
    "\n",
    "            # get the stistics value information\n",
    "            stats_values = get_stats(full_data_file, point, 'StatisticsValue')\n",
    "\n",
    "            # get the track and object id information in one np array\n",
    "            track_and_object_id_info = convert_to_matrix(track_id_data, track_object_data)\n",
    "\n",
    "            # create a dict to extract the data \n",
    "            stats_data = extract_data(track_and_object_id_info, stats_values)\n",
    "\n",
    "            # invert the stats dict ie: swap key and values\n",
    "            inv_stats_dict = invert_stats_dict(stats_dict)\n",
    "\n",
    "            # initialize the class to create all the necessary columns\n",
    "            statscols = StatsColumns(\n",
    "                idx,\n",
    "                stats_dict,\n",
    "                track_id_data,\n",
    "                track_object_data,\n",
    "                stats_values,\n",
    "                track_and_object_id_info,\n",
    "                stats_data,\n",
    "                inv_stats_dict)\n",
    "\n",
    "            # get the number of object in current point\n",
    "            num_points = statscols.obj_ids.shape[0]\n",
    "\n",
    "            # create a empty storage dict to store the data from each point\n",
    "            storage_dict = {}\n",
    "\n",
    "            # update metadata\n",
    "            metadata_storage[point] = {'num_obj_ids': num_points, 'num_track_ids': statscols.track_and_object_id_info.shape[0]}\n",
    "            \n",
    "            # grab the special items\n",
    "            for key in functions_dict.keys():\n",
    "\n",
    "                if type(functions_dict[key]) == list: \n",
    "                    storage_dict[key] = getattr(statscols, 'universal_create_track_channel_value_column')(*functions_dict[key])\n",
    "                else:\n",
    "                    if key not in config['special_items']:\n",
    "                        storage_dict[key] = getattr(statscols, 'universal_create_stats_column')(functions_dict[key])\n",
    "                    else:\n",
    "                        storage_dict[key] = getattr(statscols, functions_dict[key])()\n",
    "                        \n",
    "            full_storage[point] = storage_dict\n",
    "\n",
    "            # update dataframe\n",
    "            points_data_arr = pd.DataFrame(\n",
    "                data=np.hstack(list(full_storage[point].values())),\n",
    "                columns=list(functions_dict.keys()))\n",
    "\n",
    "            dataframe_storage.append(points_data_arr)\n",
    "\n",
    "            #print(f'info: found track')\n",
    "            \n",
    "        except (KeyError, AttributeError):\n",
    "            print(f'info -- no track')\n",
    "            pass\n",
    "        \n",
    "    # concatenate all the points and return\n",
    "    return pd.concat(dataframe_storage), metadata_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e9d10-f4b8-418f-8792-fef00c0b6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def subprocess(config: dict, categories: list, drop_duplicates: bool, data_path: str, keep_id: bool):\n",
    "    '''\n",
    "    args:\n",
    "        config_path [str] -- path to the config yaml file.\n",
    "        drop_duplicates [bool] -- if True keeps only track information, if False keeps information for every object in each track.\n",
    "        keep_id [bool] -- if True keeps the track ID name if False, removes track id names from generated csv file.\n",
    "        config [dict] -- configuration dictionary\n",
    "        categories [list] -- a list containing all the statistics categories\n",
    "        data_path [str] -- path to the imaris file\n",
    "    function:\n",
    "        this function is used with ray, it runs the entire pipeline for a single imaris file\n",
    "        it generates all the statistics for each point in the imaris file and saves it to a csv\n",
    "    returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        #print(f\"\\ninfo: data path -- {data_path}\")\n",
    "        \n",
    "        # get the name of the imaris file\n",
    "        imaris_name = os.path.basename(data_path).split('.')[0]\n",
    "        # create the csv file name\n",
    "        csv_name = f\"{imaris_name}.csv\"\n",
    "        # create the metadata file name\n",
    "        metadata_name = f\"{imaris_name}.yaml\"\n",
    "\n",
    "        # generate data \n",
    "        data_frame, metadata = run(config, data_path, categories, drop_duplicates)\n",
    "\n",
    "        # remove unwanted columns with NO/EMPTY values\n",
    "        data_frame.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "        # save data_frame\n",
    "        # create directory to store csv file\n",
    "        save_path = os.path.join(config['save_dir'], config['data_dir'])\n",
    "\n",
    "        # drop the duplicates and only keep info for a single track\n",
    "        if drop_duplicates:\n",
    "            # drop the duplicates and keep only the last row\n",
    "            data_frame = data_frame.drop_duplicates(subset=['ID'], keep='last', inplace=False, ignore_index=True)\n",
    "            \n",
    "        # switch to indicate whether or not to drop track id information column\n",
    "        if keep_id == False:\n",
    "            data_frame = data_frame.drop('ID', axis=1)\n",
    "            \n",
    "        # finally save the data csv\n",
    "        data_frame.to_csv(os.path.join(config['save_dir'], csv_name), index=False)\n",
    "        \n",
    "        # save the metadata yaml file\n",
    "        dict_to_yaml(metadata, os.path.join(config['save_dir'], metadata_name))\n",
    "        \n",
    "    except (ValueError, AttributeError):\n",
    "        print(f'info -- Skipping \"\"{data_path}\"\" File - No Tracks Found\\n')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b64da-f906-4d1c-8d2c-110d917f8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path: str, drop_duplicates: bool=True, keep_id: bool=False) -> None:\n",
    "    '''\n",
    "    args:\n",
    "        config_path [str] -- path to the config yaml file.\n",
    "        drop_duplicates [bool] -- if True keeps only track information, if False keeps information for every object in each track.\n",
    "        keep_id [bool] -- if True keeps the track ID name if False, removes track id names from generated csv file.\n",
    "    '''\n",
    "    # load yaml file as a dictionary\n",
    "    config = load_yaml(config_path)\n",
    "    \n",
    "    # get the statistics categories\n",
    "    categories = read_txt(config['stats_category_path'])\n",
    "    \n",
    "    # loop over each data directory\n",
    "    for data_dir in config['data_dir']:\n",
    "        # get the name of the save folder\n",
    "        save_folder = os.path.basename(os.path.normpath(data_dir))\n",
    "        save_dir_path = os.path.join(config['save_dir'], save_folder)\n",
    "        \n",
    "        # create the folder if its not there\n",
    "        if not os.path.exists(save_dir_path):\n",
    "            os.mkdir(save_dir_path)\n",
    "        \n",
    "        # create a new copy of the config file so it has a single data_dir and single save_dir\n",
    "        temp_config = copy.deepcopy(config)\n",
    "        temp_config['data_dir'] = data_dir\n",
    "        temp_config['save_dir'] = save_dir_path\n",
    "        \n",
    "        # run the section below as before on the new config file\n",
    "        # get all the imaris files in the directory\n",
    "        data_paths = glob.glob(os.path.join(temp_config['data_dir'], '*.ims'))\n",
    "\n",
    "        # create an empty list to store all the subprocesses to be executed by ray\n",
    "        processes = []\n",
    "\n",
    "        # apped each function to be executed to the list\n",
    "        print('info -- generating subprocesses')\n",
    "        print(f'info -- subprocess being created for the following {len(data_paths)} imaris files')\n",
    "        for idx, path in enumerate(data_paths):\n",
    "            print(f'info -- file {idx} : {path}')\n",
    "            processes.append(subprocess.remote(temp_config, categories, drop_duplicates, path, keep_id))\n",
    "\n",
    "        # run ray to lauch each function in a parallel manner\n",
    "        print('info -- running subprocesses:')\n",
    "    \n",
    "        ray.get(processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c047b9-71fa-4f5b-80c6-91ad146d8650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time run\n",
    "start = time.time()\n",
    "\n",
    "# configuration path\n",
    "config_path = 'config.yaml'\n",
    "\n",
    "# drop duplicates True if you want to keep only track wise info\n",
    "# drop duplicates False if you want to keep object wise info\n",
    "drop_duplicates = True\n",
    "\n",
    "# if True saves the track ID number, if False discards track ID from csv\n",
    "keep_id = True\n",
    "\n",
    "# run \n",
    "main(config_path=config_path, drop_duplicates=drop_duplicates, keep_id=keep_id)\n",
    "\n",
    "# print time\n",
    "end = time.time()\n",
    "print(f\"[info] -- finished folder in {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe9673-3190-4f20-98eb-01c04b98e2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43639789-c0c6-4099-8adc-d4a384c0b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_ray_file -- runs each folder sequentially but runs each file within the folder as a subprocess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
