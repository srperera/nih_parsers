{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notes\n",
    "    * Filename: launch_parser_mp_files.ipynb\n",
    "    * Multiprocessing performed for each directory\n",
    "    * Example: For all files in each directory all surfaces for every file are executed in parallel\n",
    "    * High Memory Consumption IF imaris files are large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import ray\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from utils import utils\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Available Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_available_categories(config_path: str, save_path: str):\n",
    "    # load config path\n",
    "    yaml = utils.load_yaml(config_path)\n",
    "\n",
    "    # type of parser\n",
    "    parser_type = yaml[\"parser_type\"]\n",
    "\n",
    "    # extract category func\n",
    "    extract_categories = utils.get_category_function(parser_type)\n",
    "\n",
    "    # files to scan\n",
    "    directories = yaml[\"data_dir\"]\n",
    "\n",
    "    # valid surface\n",
    "    valid_surface = int(yaml[\"valid_surface\"][0]) - 1\n",
    "\n",
    "    for directory in directories:\n",
    "        # grab all the files in the directory w/ .ims\n",
    "        filenames = list(glob.glob(os.path.join(directory, \"*.ims\")))\n",
    "\n",
    "        for filename in filenames:\n",
    "            # file path\n",
    "            file_path = filename\n",
    "\n",
    "            # get and save the available categories csv file\n",
    "            extract_categories(file_path, valid_surface, save_path)\n",
    "\n",
    "            break\n",
    "\n",
    "        break\n",
    "\n",
    "    print(\"[info] Please Edit The Statistics File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN GENERATE CATEGORIES\n",
    "# generate_available_categories(\"configs/config.yaml\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_categories = utils.read_txt('stats_categories_track.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(config_path: str):\n",
    "    # init ray\n",
    "    ray.init(\n",
    "        ignore_reinit_error=True,\n",
    "    )\n",
    "\n",
    "    chunk_size = 4\n",
    "\n",
    "    # load config path\n",
    "    yaml = utils.load_yaml(config_path)\n",
    "\n",
    "    # type of parser\n",
    "    parser_type = yaml[\"parser_type\"]\n",
    "\n",
    "    if parser_type == \"track\":\n",
    "        from parsers.track_parser import extract_data, process_and_save\n",
    "\n",
    "        print(f\"[info] -- parser mode: {parser_type}\")\n",
    "\n",
    "    elif parser_type == \"surface\":\n",
    "        from parsers.surface_parser import extract_data, process_and_save\n",
    "\n",
    "        print(f\"[info] -- parser mode: {parser_type}\")\n",
    "\n",
    "    elif parser_type == \"first\":\n",
    "        from parsers.first_surface_parser import extract_data, process_and_save\n",
    "\n",
    "        print(f\"[info] -- parser mode: {parser_type}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Parser Type\")\n",
    "\n",
    "    # files to scan\n",
    "    directories = yaml[\"data_dir\"]\n",
    "\n",
    "    # get the stats categories\n",
    "    # stats_categories = utils.read_txt(yaml[\"stats_category_path\"])\n",
    "\n",
    "    # valid surface\n",
    "    valid_surfaces = yaml[\"valid_surface\"]\n",
    "\n",
    "    # iterate, process and save\n",
    "    # parallel processes all files within 1 directory at at time\n",
    "    skip_list = []\n",
    "    for idx, directory in tqdm(enumerate(directories)):\n",
    "        print(f\"[info] -- processing directory index: {idx+1}/{len(directories)}\")\n",
    "\n",
    "        # save dir\n",
    "        save_dir = yaml[\"save_dir\"][idx]\n",
    "\n",
    "        # grab all the files in the directory w/ .ims\n",
    "        filenames = list(glob.glob(os.path.join(directory, \"*.ims\")))\n",
    "        print(\"Len Files: \", len(filenames), filenames[0])\n",
    "\n",
    "        filenames = np.array(filenames, dtype=str)\n",
    "        num_chunks = np.ceil(len(filenames) / chunk_size)\n",
    "        file_chunks = np.array_split(filenames, num_chunks)\n",
    "\n",
    "        # create a list to hold ray subprocess\n",
    "\n",
    "        for filenames in file_chunks:\n",
    "            processes = []\n",
    "\n",
    "            for filename in filenames.tolist():\n",
    "                try:\n",
    "                    print(f\"\\n[info] -- processing file {os.path.basename(filename)}\\n\")\n",
    "\n",
    "                    # load the imaris file\n",
    "                    data = utils.load_ims(filename)\n",
    "\n",
    "                    for surface in valid_surfaces:\n",
    "                        # get the stats categories available for current surface\n",
    "                        extract_categories = utils.get_category_function(parser_type)\n",
    "                        stats_categories = extract_categories(\n",
    "                            filename, int(surface), save_path=\".\"\n",
    "                        )\n",
    "                        # create folder\n",
    "                        folder_path = os.path.join(save_dir, str(surface))\n",
    "                        if not os.path.isdir(folder_path):\n",
    "                            os.makedirs(folder_path)\n",
    "\n",
    "                        # convert to zero indexed surface value\n",
    "                        current_surface = int(surface) - 1\n",
    "\n",
    "                        # save_file_path\n",
    "                        save_path = utils.get_save_filepath(\n",
    "                            parser_type, folder_path, filename, (current_surface + 1)\n",
    "                        )\n",
    "\n",
    "                        # extract data\n",
    "                        extracted_data = extract_data(\n",
    "                            filename, data, current_surface, save_path\n",
    "                        )\n",
    "\n",
    "                        # process and save\n",
    "                        if extracted_data:\n",
    "                            # append stats categories to extracted data\n",
    "                            extracted_data.update({\"categories_list\": stats_categories})\n",
    "                            task_ref = process_and_save.remote(extracted_data)\n",
    "                            processes.append(task_ref)\n",
    "\n",
    "                        else:\n",
    "                            # there is no surface deleting folder\n",
    "                            if not os.listdir(folder_path):\n",
    "                                os.rmdir(folder_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    # for any reason it cannot execute a file it will skip that file\n",
    "                    print(f\"\\t[info] -- error file: {filename}\")\n",
    "                    print(f\"\\t[info] -- raised Exception [{e}]\")\n",
    "                    print(f\"\\t[info] -- skipping file\\n\")\n",
    "                    skip_list.append(filename)\n",
    "                    pass\n",
    "\n",
    "            print(\"\\n\\t[info] -- finished data extraction\")\n",
    "            print(f\"\\t[info] -- found {len(processes)} surfaces\")\n",
    "            print(f\"\\t[info] -- processing {len(processes)} surfaces\\n\")\n",
    "\n",
    "            ray.wait(processes, num_returns=len(processes))\n",
    "\n",
    "            # memory clean up\n",
    "            del data\n",
    "            del processes\n",
    "            gc.collect()\n",
    "\n",
    "    np.savetxt(\"skipped_files.txt\", np.array(skip_list, dtype=str), fmt=\"%s\")\n",
    "\n",
    "    ray.shutdown()\n",
    "    print(f\"\\n[info] -- DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] -- parser mode: first\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211342003a984d2ab7c5bb0163e6acae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] -- processing directory index: 1/1\n",
      "Len Files:  3 ../../data/multi_surface_track_parser_dev_data/GFP #1 Sec1 Roi2 2x2 1h30min.ims\n",
      "\n",
      "[info] -- processing file GFP #1 Sec1 Roi2 2x2 1h30min.ims\n",
      "\n",
      "\t[info] extracting data: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface: 0\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 0\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 0 -- processed 1098 items\n",
      "\t[info] extracting data: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface: 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 1 -- processed 293 items\n",
      "\t[info] extracting data: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface: 2\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 2\n",
      "\t[info] extracting data: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface: 3\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 2 -- processed 558 items\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 3\n",
      "\t[info] extracting data: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface: 4\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 3 -- processed 233 items\n",
      "\n",
      "[info] -- processing file YFP #1 Sec2 Roi1 2x2 1h shehan.ims\n",
      "\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 4\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec1 Roi2 2x2 1h30min.ims -- surface 4 -- processed 129 items\n",
      "\t[info] extracting data: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface: 0\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 0\n",
      "\t[info] extracting data: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface: 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 0 -- processed 1860 items\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 1 -- processed 1028 items\n",
      "\t[info] extracting data: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface: 2\n",
      "\t[info] -- error file: ../../data/multi_surface_track_parser_dev_data/YFP #1 Sec2 Roi1 2x2 1h shehan.ims\n",
      "\t[info] -- raised Exception [list index out of range]\n",
      "\t[info] -- skipping file\n",
      "\n",
      "\n",
      "[info] -- processing file GFP #1 Sec2 Roi1 2x2 1h30min.ims\n",
      "\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 2\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: YFP #1 Sec2 Roi1 2x2 1h shehan.ims -- surface 2 -- processed 555 items\n",
      "\t[info] extracting data: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface: 0\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 0\n",
      "\t[info] extracting data: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface: 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 0 -- processed 1234 items\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] working on file: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 1\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] finished: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 1 -- processed 257 items\n",
      "\t[info] extracting data: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface: 2\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] working on file: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 2\n",
      "\t[info] extracting data: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface: 3\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] finished: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 2 -- processed 150 items\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] working on file: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 3\n",
      "\t[info] extracting data: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface: 4\n",
      "\n",
      "\t[info] -- finished data extraction\n",
      "\t[info] -- found 13 surfaces\n",
      "\t[info] -- processing 13 surfaces\n",
      "\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] working on file: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 4\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209975)\u001b[0m \t[info] finished: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 4 -- processed 36 items\n",
      "\u001b[2m\u001b[36m(process_and_save pid=209959)\u001b[0m \t[info] finished: GFP #1 Sec2 Roi1 2x2 1h30min.ims -- surface 3 -- processed 1172 items\n",
      "\n",
      "[info] -- DONE\n",
      "Total Run Time: 24.607701265020296\n"
     ]
    }
   ],
   "source": [
    "# RUN GENERATE STATISTICS\n",
    "start = time.perf_counter()\n",
    "generate_statistics(config_path=\"configs/config_first.yaml\")\n",
    "stop = time.perf_counter()\n",
    "print(f\"Total Run Time: {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "04aae946aefb73757d94e4e7bfb7ede85957aa0149a3e71cc39e2fcc66c8e46e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
